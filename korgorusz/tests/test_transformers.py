# from korgorusz.transformers import attention


# def test_multiheaded_attention():
#     mha = attention.MultiHeadAttention(8, 512, 64, 64)
#     assert False


# def test_multiheaded_attention_mask():
#     assert False
